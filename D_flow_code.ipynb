{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USeN1h7bHp3m"
      },
      "outputs": [],
      "source": [
        "# get the contents of the github so we can import the FM model\n",
        "!git clone https://github.com/atong01/conditional-flow-matching.git\n",
        "%cd conditional-flow-matching\n",
        "!pip install -r requirements.txt\n",
        "!pip install torchdiffeq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Imports\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Standard Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import yaml\n",
        "from typing import KeysView\n",
        "\n",
        "\n",
        "# Data Storage imports\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# DL imports\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import GaussianBlur\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchmetrics.image import lpip, PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure # lpip, PSNR, SSIM\n",
        "\n",
        "\n",
        "# ODE imports\n",
        "from torchdyn.core import NeuralODE\n",
        "import torchdiffeq\n",
        "\n",
        "# Plot imports\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.transforms import ToPILImage\n",
        "import PIL.Image\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "\n",
        "# Github imports\n",
        "from torchcfm.conditional_flow_matching import (\n",
        "    ConditionalFlowMatcher,\n",
        "    ExactOptimalTransportConditionalFlowMatcher,\n",
        "    TargetConditionalFlowMatcher,\n",
        "    VariancePreservingConditionalFlowMatcher,\n",
        ")\n",
        "from torchcfm.models import MLP\n",
        "from torchcfm.models.unet.unet import UNetModelWrapper\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Global Constants\n",
        "# ------------------------------------------------------------------------------\n",
        "# Split into sub dictionaries for better organisation, or some other config type like yaml files.\n",
        "\n",
        "CONFIG = {\n",
        "    'flow_model': 'otcfm',\n",
        "    'model_variant': 'normal', # normal or ema\n",
        "    'output_dir': './results/',\n",
        "    'models_dir': '/content/drive/MyDrive/KTH/DD2412 Deep Learning Adv/Project/flow_models/',\n",
        "    'in_channels': 3,\n",
        "    'sample_size': 32,\n",
        "    'class_cond': False,    # cant use class conditioning currently as none of the pt files contains label_emb but the UNet contains this variable if class_cond==True\n",
        "    'num_classes': 10,\n",
        "    'y_class': 0,\n",
        "    'N': 20,\n",
        "    'corrupt_Type': 'inpainting',\n",
        "    'ODE_steps': 100,\n",
        "    'lr': 1,\n",
        "    'lr_type': 'constant',\n",
        "    'inpaint_percent': 0.9,\n",
        "    'use_checkpointing': True,\n",
        "    'frozen_model': False,\n",
        "    'ODE_type': 'dopri5',\n",
        "    'gamma': 1,\n",
        "    'blend_param': 0.1,\n",
        "    'FID_batch': 16,\n",
        "    'FID_num_samples': 16,\n",
        "    'num_images':5,\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Just here for now for debugging purposes, so even if d-flow crashes we can still access the intermediate results\n",
        "x_0_list = []\n",
        "image_list = []\n",
        "losses = []\n"
      ],
      "metadata": {
        "id": "KOl__zuYH6rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_output(path, _dict):\n",
        "  torch.save(_dict, path)\n",
        "\n",
        "def load_output(path):\n",
        "  return torch.load(path, weights_only=False, map_location=device)\n",
        "\n",
        "def save_config(config, save_dir):\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "  config_path = os.path.join(save_dir, 'config.yaml')\n",
        "  with open(config_path, 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "  #images = np.load(os.path.join(output_dir, 'images.npy'))\n",
        "  #losses = np.load(os.path.join(output_dir, 'losses.npy'))\n",
        "\n",
        "def load_model(device):\n",
        "  \"\"\"\n",
        "  Checkpoint contains:\n",
        "    net_model\n",
        "    ema_model\n",
        "    sched\n",
        "    optim\n",
        "    step\n",
        "  We only need net_model for inference, as the rest are for training the flow model.\n",
        "  \"\"\"\n",
        "  # Create directory where all results will be stored\n",
        "  save_dir = os.path.join(CONFIG['output_dir'], CONFIG['flow_model'])\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "  # Load checkpoint\n",
        "  try:\n",
        "    checkpoint_path = os.path.join(\n",
        "      CONFIG['models_dir'], f\"{CONFIG['flow_model']}_cifar10_weights_step_400000.pt\"\n",
        "    )\n",
        "    print(f\"Attempting to load {CONFIG['flow_model']} model from {checkpoint_path}\")\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, weights_only=True, map_location=device)\n",
        "  except NotImplementedError:\n",
        "    raise NotImplementedError(\n",
        "      f\"Unknown model {CONFIG['flow_model']}, must be one of ['otcfm', 'icfm', 'fm']\"\n",
        "    )\n",
        "  except FileNotFoundError:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Checkpoint file not found at {checkpoint_path}. Verify the path or the file's existence.\"\n",
        "    )\n",
        "\n",
        "  # Create UNet model and load in it's corresponding weights\n",
        "  net_model = net_model = UNetModelWrapper(\n",
        "    dim=(CONFIG['in_channels'], CONFIG['sample_size'], CONFIG['sample_size']),\n",
        "    class_cond=CONFIG['class_cond'],\n",
        "    num_classes=CONFIG['num_classes'],\n",
        "    num_res_blocks=2,\n",
        "    num_channels=128,\n",
        "    channel_mult=[1, 2, 2, 2],\n",
        "    num_heads=4,\n",
        "    num_head_channels=64,\n",
        "    attention_resolutions=\"16\",\n",
        "    dropout=0.1,\n",
        "    use_checkpoint=CONFIG['use_checkpointing'],\n",
        "  ).to(device)\n",
        "\n",
        "  if CONFIG['model_variant'] == 'normal':\n",
        "    net_model.load_state_dict(checkpoint['net_model'])\n",
        "  elif CONFIG['model_variant'] == 'ema':\n",
        "    net_model.load_state_dict(checkpoint['ema_model'])\n",
        "\n",
        "  if CONFIG['frozen_model']:\n",
        "    for name, param in net_model.named_parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  print(f\"Succesfully loaded the {CONFIG['model_variant']} {CONFIG['flow_model']} model onto the {device}.\")\n",
        "  if device == 'cpu': print(f\"Consider switching device to GPU, as it will run out of RAM during D-flow otherwise.\")\n",
        "  return net_model"
      ],
      "metadata": {
        "id": "LxC9vzxFH1n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Corruptions\n",
        "# ------------------------------------------------------------------------------\n",
        "def inpainting_corruption(image):\n",
        "  painted_image = torch.clone(image)\n",
        "  painted_image[0, :, 14:18, 14:18] = 1\n",
        "  return painted_image\n",
        "\n",
        "def blur_corruption(image):\n",
        "  #image_copy = torch.clone(image)\n",
        "  filter_kernel = GaussianBlur(3, sigma=1)\n",
        "  blurred_image = filter_kernel(image)\n",
        "  return blurred_image\n",
        "\n",
        "def resolution_corruption():\n",
        "  pass\n",
        "\n",
        "def corrupt_image(image, corruption_type):\n",
        "  \"\"\"\n",
        "  Function to corrupt the image\n",
        "  \"\"\"\n",
        "  if corruption_type == \"blur\":\n",
        "    return blur_corruption(image)\n",
        "  elif corruption_type == \"inpainting\":\n",
        "    return inpainting_corruption(image)\n",
        "  elif corruption_type == \"resolution\":\n",
        "    return resolution_corruption(image)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Loss Functions\n",
        "# ------------------------------------------------------------------------------\n",
        "def get_mask(x, percentage=0.9):\n",
        "  _,_, H, W = x.shape\n",
        "\n",
        "  # Create a mask that is True outside the painted region\n",
        "  mask = torch.ones((H, W))\n",
        "  mask[14:18, 14:18] = 0\n",
        "\n",
        "  # Flatten H and W to one dimension so we can more easily subsample from them\n",
        "  mask_flat = mask.view(H * W)\n",
        "\n",
        "  # Get indices of valid (unpainted) pixels and sample from them\n",
        "  valid_indices = torch.nonzero(mask_flat).squeeze()\n",
        "  sampled_indices = np.random.choice(valid_indices.numpy(), size=int(len(valid_indices) * percentage), replace=False)\n",
        "  return sampled_indices\n",
        "\n",
        "def inpainting_loss(x,y,mask):\n",
        "  B, C, H, W = x.shape\n",
        "\n",
        "  # Flatten the image as the mask is flat\n",
        "  x_flat, y_flat = x.view(B, C, H*W), y.view(B, C, H*W)\n",
        "\n",
        "  # Retrieve the values at the sampled indices\n",
        "  x_sampled_pixels = x_flat[:, :, mask]\n",
        "  y_sampled_pixels = y_flat[:, :, mask]\n",
        "\n",
        "  # Return the loss\n",
        "  return torch.mean((x_sampled_pixels - y_sampled_pixels)**2)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Plots\n",
        "# ------------------------------------------------------------------------------\n",
        "def display_sample(sample, i):\n",
        "  image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "  image_processed = (image_processed + 1.0) * 127.5\n",
        "  image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "  image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "  display(f\"Image at step {i}\")\n",
        "  display(image_pil)\n",
        "\n",
        "def plot_d_flow_process(x_1_list, img_per_row):\n",
        "  test_images = []\n",
        "  for i, images in enumerate(x_1_list):\n",
        "    images = images[-1].view([3, 32, 32]).clip(-1, 1)\n",
        "    images = images / 2 + 0.5\n",
        "    test_images.append(images)\n",
        "\n",
        "  grid = make_grid(test_images, value_range=(0, 1), nrow=img_per_row, padding=2)\n",
        "  img = ToPILImage()(grid)\n",
        "  plt.imshow(img)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_losses(losses):\n",
        "  all_loses = []\n",
        "  for sublist in losses:\n",
        "    for loss in sublist:\n",
        "      all_loses.append(loss.item())\n",
        "\n",
        "  # Plot the loss curve\n",
        "  plt.plot(all_loses)\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(f\"Loss vs Iteration\")\n",
        "  plt.show()\n",
        "\n",
        "def denormalize_images(images):\n",
        "  if isinstance(images, list):\n",
        "    if images[0].ndim == 4:\n",
        "      images = [img.squeeze(0) for img in images]\n",
        "    return [img / 2 + 0.5 for img in images]\n",
        "  return images / 2 + 0.5\n",
        "\n",
        "def create_img_grid(images, _nrow=4, _padding=2, plot=False):\n",
        "  grid = make_grid(denormalize_images(images), value_range=(0,1), nrow=_nrow, padding=_padding)\n",
        "  if plot:\n",
        "    img = ToPILImage()(grid)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "  return grid"
      ],
      "metadata": {
        "id": "iZNJknzgIO4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_x_0(model, corrupted_y):\n",
        "  x_0 = torch.randn(\n",
        "   1, CONFIG['in_channels'], CONFIG['sample_size'], CONFIG['sample_size'], device=device, requires_grad=True\n",
        "  )\n",
        "  ODE_func = lambda t, x: model.forward(t, x)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    x_0 = torchdiffeq.odeint(\n",
        "      func=ODE_func,\n",
        "      y0=corrupted_y,\n",
        "      t=torch.linspace(1, 0, CONFIG['ODE_steps'], device=device),\n",
        "      atol=1e-4,\n",
        "      rtol=1e-4,\n",
        "      method=CONFIG['ODE_type'],\n",
        "    )[-1].to(device) # only keep the final value from the ode solver\n",
        "\n",
        "  x_0 = CONFIG['blend_param'] **0.5 * (corrupted_y + x_0) + (1. - CONFIG['blend_param'])**0.5*torch.randn_like(x_0)\n",
        "  x_0.requires_grad=True\n",
        "  return x_0\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Our d-flow algorithm implementation\n",
        "# ------------------------------------------------------------------------------\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "def setup_experiment_dirs(base_dir, experiment_id):\n",
        "  log_dir = os.path.join(base_dir, f'experiment_{experiment_id}/logs')\n",
        "  output_dir = os.path.join(base_dir, f'experiment_{experiment_id}')\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  return log_dir, output_dir\n",
        "\n",
        "def save_PIL_images(path, output_dict):\n",
        "  os.makedirs(path, exist_ok=True)\n",
        "  D_flow_images = [output_dict[image]['x_1_list'][-1][-1] for image in output_dict.keys()]\n",
        "  denormalized_D_flow_images = denormalize_images(D_flow_images)\n",
        "\n",
        "  for idx, image_tensor in enumerate(denormalized_D_flow_images):\n",
        "    pil_image = TF.to_pil_image(image_tensor.squeeze(0))\n",
        "\n",
        "    image_file_path = os.path.join(path, f'd_flow_image_{idx}.jpg')\n",
        "    pil_image.save(image_file_path)\n",
        "\n",
        "def evaluate_d_flow(model, N, corruption_type='inpainting'):\n",
        "  # Setup experiment directories\n",
        "  base_dir = '/content/drive/MyDrive/KTH/DD2412 Deep Learning Adv/Project/Runs'\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "  experiment_id = len(os.listdir(base_dir)) + 2\n",
        "  print(f\"Experiment {experiment_id}\")\n",
        "  log_dir, output_dir = setup_experiment_dirs(base_dir, experiment_id)\n",
        "  config_file = os.path.join(base_dir, f'experiment_{experiment_id}/config.yaml')\n",
        "\n",
        "  # Save configuration\n",
        "  save_config(CONFIG, config_file)\n",
        "\n",
        "  # TensorBoard logging\n",
        "  writer = SummaryWriter(log_dir)\n",
        "\n",
        "  output_dict = {}\n",
        "  for i in range(CONFIG['num_images']):\n",
        "    output_dict[i] = {}\n",
        "\n",
        "    # Run D-flow algorithm\n",
        "    y = loaded_single_class_images[i].view(1,3,32,32).to(device)\n",
        "\n",
        "    # Store and log ground truth Y\n",
        "    test_y = y.clone()\n",
        "    test_y = test_y.view([3, 32, 32]).clip(-1, 1)\n",
        "    output_dict[i]['y'] = test_y\n",
        "    writer.add_image('Ground Truth', denormalize_images(test_y.squeeze(0)), i)\n",
        "\n",
        "    # Run D-flow\n",
        "    x_0_list, image_list, losses = d_flow(model, y, N, writer, output_dict, corruption_type, img_num=i)\n",
        "\n",
        "    # Log results\n",
        "    grid = create_img_grid([images[-1].view([3,32,32]).clip(-1,1) for images in image_list+[y]], _nrow=3, _padding=2)\n",
        "\n",
        "    writer.add_image('D-Flow Step Images', grid, i)\n",
        "\n",
        "  save_output(os.path.join(output_dir, 'output_dict.pt'), output_dict)\n",
        "  save_PIL_images(os.path.join(output_dir, 'd_flow_images'), output_dict)\n",
        "\n",
        "def d_flow(model, y, N, writer, output_dict, corruption_type='inpainting', img_num=0):\n",
        "  # Create corrupted y and x_0\n",
        "  corrupted_y = corrupt_image(y, corruption_type).to(device)\n",
        "  corrupted_y.requires_grad = False\n",
        "\n",
        "  # Store and log corrupted Y\n",
        "  test_corrupted_y = corrupted_y.clone()\n",
        "  test_corrupted_y = test_corrupted_y.view([3, 32, 32]).clip(-1, 1)\n",
        "\n",
        "  output_dict[img_num][\"Corrupted Y\"] = test_corrupted_y\n",
        "  writer.add_image('Corrupted Y', denormalize_images(test_corrupted_y), img_num)\n",
        "\n",
        "\n",
        "  ODE_func = lambda t, x: model.forward(t, x)\n",
        "\n",
        "  x_0 = generate_x_0(model, corrupted_y)\n",
        "\n",
        "  optim = torch.optim.LBFGS(\n",
        "    [x_0],\n",
        "    lr=CONFIG['lr'],\n",
        "    max_iter=CONFIG['max_iter'],\n",
        "    history_size=20,\n",
        "    tolerance_grad=1e-7,\n",
        "    tolerance_change=1e-9,\n",
        "    line_search_fn=\"strong_wolfe\"\n",
        "  )\n",
        "  scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=CONFIG['gamma'])\n",
        "\n",
        "  # Get fixed corruption function\n",
        "  mask = get_mask(corrupted_y, percentage=CONFIG['inpaint_percent'])\n",
        "\n",
        "  x_0_list, image_list, losses = [], [], []\n",
        "\n",
        "  closure_counter = 0\n",
        "  # Run D-flow algorithm\n",
        "  for i in tqdm.tqdm(range(N)):\n",
        "    iter_losses, iter_x_0s, iter_x_1s = [], [], []\n",
        "    def closure():\n",
        "      optim.zero_grad()\n",
        "\n",
        "      # Solve ODE\n",
        "      x_1 = torchdiffeq.odeint(\n",
        "        func=ODE_func,\n",
        "        y0=x_0,\n",
        "        t=torch.linspace(0, 1, CONFIG['ODE_steps'], device=device),\n",
        "        atol=1e-4,\n",
        "        rtol=1e-4,\n",
        "        method=CONFIG['ODE_type'],\n",
        "      )[-1] # only keep the final value from the ode solver\n",
        "\n",
        "      # Compute loss\n",
        "      loss = inpainting_loss(x_1, corrupted_y, mask)\n",
        "\n",
        "      # Add elements to lists for debugg and plots\n",
        "      iter_losses.append(loss.item())\n",
        "      iter_x_0s.append(x_0.detach())\n",
        "      iter_x_1s.append(x_1.detach())\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_([x_0], max_norm=1)\n",
        "      return loss\n",
        "\n",
        "    optim.step(closure)\n",
        "    scheduler.step()\n",
        "\n",
        "    losses.append(iter_losses)\n",
        "    x_0_list.append(iter_x_0s)\n",
        "    image_list.append(iter_x_1s)\n",
        "\n",
        "    # Logging code\n",
        "    grid = create_img_grid([image.view([3,32,32]).clip(-1,1) for image in iter_x_1s])\n",
        "\n",
        "    writer.add_image(f'Closure Images/img_{img_num}', grid, closure_counter)\n",
        "    closure_counter += 1\n",
        "\n",
        "  output_dict[img_num]['x_0_list'] = x_0_list\n",
        "  output_dict[img_num]['x_1_list'] = image_list\n",
        "  output_dict[img_num]['losses'] = losses\n",
        "  return x_0_list, image_list, losses\n"
      ],
      "metadata": {
        "id": "XYiHmBrXIcUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"/content/drive/MyDrive/KTH/DD2412 Deep Learning Adv/Project/Runs\""
      ],
      "metadata": {
        "id": "PUjZ7B3oIh2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change CONFIG as needed, then run\n",
        "net_model=load_model(device)\n",
        "\n",
        "evaluate_d_flow(net_model, 5, corruption_type=CONFIG['corrupt_Type'])"
      ],
      "metadata": {
        "id": "vbpIWlG0IfAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_output(path, device):\n",
        "  return torch.load(path, weights_only=False, map_location=device)\n",
        "\n",
        "def eval_model(true_data_dir, d_flow_output_path, N_patches=8):\n",
        "  # output_dict[img_num]['x_0_list'], utput_dict[img_num]['x_1_list'], output_dict[img_num]['losses'],  output_dict[img_num]['y'],  output_dict[img_num]['Corrupted Y']\n",
        "  single_class_images_path = os.path.join(true_data_dir, \"single_class_images.pt\")\n",
        "\n",
        "  output_dict = load_output(d_flow_output_path, \"cpu\")\n",
        "  single_class_images = load_output(single_class_images_path, \"cpu\")\n",
        "\n",
        "  final_d_flow_images = [output_dict[image]['x_1_list'][-1][-1].clip(-1,1) for image in output_dict.keys()]\n",
        "  single_class_images = [img.unsqueeze(0) for img in single_class_images.values()]\n",
        "\n",
        "  lpip_metric = lpip.LearnedPerceptualImagePatchSimilarity(net_type='squeeze').to(device)\n",
        "  psnr_metric = PeakSignalNoiseRatio().to(device)\n",
        "  ssim_metric = StructuralSimilarityIndexMeasure(data_range=2.0).to(device)  # Data range for [-1, 1]\n",
        "\n",
        "  for image_pair in zip(final_d_flow_images, single_class_images):\n",
        "    patches_img_1 = image_pair[0].reshape(4,3, 16, 16)\n",
        "    patches_img_2 = image_pair[1].reshape(4,3, 16, 16)\n",
        "\n",
        "    lpip_metric.update(*image_pair)\n",
        "    psnr_metric.update(*image_pair)\n",
        "    ssim_metric.update(*image_pair)\n",
        "\n",
        "  eval_dict = {\n",
        "    \"LPIPS_sum_total\": lpip_metric.sum_scores,\n",
        "    \"LPIPS_total\": lpip_metric.total,\n",
        "    \"SSIM similarity\": ssim_metric.similarity / 5,  # sums up the similariy of all images, so / 5 is the mean\n",
        "    \"SSIM total\": ssim_metric.total,\n",
        "    \"PSNR sum_squared_error\": psnr_metric.sum_squared_error / 5, # sums up the similariy of all images, so / 5 is the mean\n",
        "    \"PSNR total\": psnr_metric.total,\n",
        "    \"PSNR value\": 20 * np.log10(255) - 10 * np.log10(psnr_metric.sum_squared_error / 5)\n",
        "  }\n",
        "\n",
        "  return eval_dict\n",
        "\n",
        "# Specify the experiments you want to evaluate\n",
        "exp_nums=[97,84,122,247,134,140, 135,139]\n",
        "final_images = []\n",
        "PATH = \"PATH\"\n",
        "DATA_PATH = \"DATA_PATH\"\n",
        "\n",
        "# Get GT and corrupted GT\n",
        "loaded_output = load_output(PATH, device)\n",
        "for key in loaded_output.keys():\n",
        "  final_images.append(loaded_output[key]['y'].unsqueeze(0))\n",
        "for key in loaded_output.keys():\n",
        "  final_images.append(loaded_output[key]['Corrupted Y'].unsqueeze(0))\n",
        "\n",
        "# Evaluate experiments and add final d-flow images\n",
        "for i, exp in enumerate(exp_nums):\n",
        "  print(\"\\n----------------------------------\")\n",
        "  print(f\"Experiment {exp}\")\n",
        "  eval_dict =eval_model(DATA_PATH, DATA_PATH)\n",
        "  for key in eval_dict:\n",
        "    print(f\"{key:} {eval_dict[key].f}\")\n",
        "  loaded_output = load_output(PATH, device)\n",
        "  for j in range(5):\n",
        "    final_images.append(loaded_output[j]['x_1_list'][-1][-1].clip(-1,1))\n",
        "\n",
        "# Plot images\n",
        "grid = create_img_grid(final_images, _nrow=5, _padding=1, plot=True)\n"
      ],
      "metadata": {
        "id": "RWN_vAG-Izae"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}