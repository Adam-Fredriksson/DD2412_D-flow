{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt4icVs6K8XZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/atong01/conditional-flow-matching.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd conditional-flow-matching\n"
      ],
      "metadata": {
        "id": "3YzomMnmLChW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "U8GZJvujLDnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq\n",
        "!pip install torch_optimizer"
      ],
      "metadata": {
        "id": "bLRsKwdoLGZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import KeysView\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.transforms import ToPILImage\n",
        "import PIL.Image\n",
        "from IPython.display import display\n",
        "import os\n",
        "import torch_optimizer\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def save_output(path, _dict):\n",
        "  torch.save(_dict, path)\n",
        "\n",
        "def load_output(path):\n",
        "  return torch.load(path, weights_only=False, map_location=device)\n",
        "\n",
        "def denormalize_images(images):\n",
        "  if isinstance(images, list):\n",
        "    if images[0].ndim == 4:\n",
        "      images = [img.squeeze(0) for img in images]\n",
        "    return [img / 2 + 0.5 for img in images]\n",
        "  return images / 2 + 0.5\n",
        "\n",
        "def create_img_grid(images, _nrow=4, _padding=2, plot=False):\n",
        "  grid = make_grid(denormalize_images(images), value_range=(0,1), nrow=_nrow, padding=_padding)\n",
        "  if plot:\n",
        "    img = ToPILImage()(grid)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "  return grid\n",
        "\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/Mac pro dator googl drive/Universitet/KTH/DD2412/Project/CIFAR10_data'\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        ")\n",
        "\n",
        "classes = [0,2,3,5,9]\n",
        "class_images = {class_id: [] for class_id in range(len(classes))}\n",
        "\n",
        "\n",
        "for img, label in testset:\n",
        "  if label in classes:\n",
        "    class_images[classes.index(label)].append(img)\n",
        "\n",
        "single_class_images = {key: images[1] for key, images in class_images.items()}\n",
        "\n",
        "create_img_grid(list(single_class_images.values()), _nrow=1, _padding=2, plot=True)\n",
        "\n",
        "subset_all_images = [img for images in class_images.values() for img in images[:10]]\n",
        "create_img_grid(subset_all_images, _nrow=10, _padding=2, plot=True)\n",
        "\n",
        "\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "class_images_path = os.path.join(save_dir, \"class_images.pt\")\n",
        "single_class_images_path = os.path.join(save_dir, \"single_class_images.pt\")\n",
        "\n",
        "for key in single_class_images.keys():\n",
        "  print(key)\n",
        "save_output(class_images_path, class_images)\n",
        "save_output(single_class_images_path, single_class_images)\n",
        "\n"
      ],
      "metadata": {
        "id": "ulgdPbTLLISE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import yaml\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import GaussianBlur\n",
        "from torchdyn.core import NeuralODE\n",
        "import torchdiffeq\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.transforms import ToPILImage\n",
        "import PIL.Image\n",
        "from IPython.display import display\n",
        "from torchcfm.conditional_flow_matching import (\n",
        "    ConditionalFlowMatcher,\n",
        "    ExactOptimalTransportConditionalFlowMatcher,\n",
        "    TargetConditionalFlowMatcher,\n",
        "    VariancePreservingConditionalFlowMatcher,\n",
        ")\n",
        "from torchcfm.models import MLP\n",
        "from torchcfm.models.unet.unet import UNetModelWrapper\n",
        "\n",
        "CONFIG = {\n",
        "    'flow_model': 'otcfm',\n",
        "    'model_variant': 'normal',\n",
        "    'output_dir': './results/',\n",
        "    'models_dir': '/content/drive/MyDrive/Mac pro dator googl drive/Universitet/KTH/DD2412/Project/flow_models',\n",
        "    'in_channels': 3,\n",
        "    'sample_size': 32,\n",
        "    'class_cond': False,\n",
        "    'num_classes': 10,\n",
        "    'y_class': 0,\n",
        "    'N': 20,\n",
        "    'corrupt_Type': 'inpainting',\n",
        "    'ODE_steps': 100,\n",
        "    'lr': 1,\n",
        "    'lr_list': [1,1,1,1,1],\n",
        "    'lr_type': 'constant',\n",
        "    'inpaint_percent': 0.9,\n",
        "    'use_checkpointing': True,\n",
        "    'frozen_model': False,\n",
        "    'ODE_type': 'dopri5',\n",
        "    'gamma': 1,\n",
        "    'blend_param': 0.1,\n",
        "    'FID_batch': 16,\n",
        "    'FID_num_samples': 16,\n",
        "    'num_images':5,\n",
        "    'inpaint_param':3,\n",
        "    'max_iter':5,\n",
        "    'lr_start':1,\n",
        "    'lr-end':0.4,\n",
        "    'total_iter':30\n",
        "\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "x_0_list = []\n",
        "image_list = []\n",
        "losses = []\n"
      ],
      "metadata": {
        "id": "O9lUXnOwLJ1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_output(path, _dict):\n",
        "  torch.save(_dict, path)\n",
        "\n",
        "def load_output(path, device):\n",
        "  return torch.load(path, weights_only=False, map_location=device)\n",
        "\n"
      ],
      "metadata": {
        "id": "4TvGJzO-LM51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_config(config, save_dir):\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "  config_path = os.path.join(save_dir, 'config.yaml')\n",
        "  with open(config_path, 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "  #images = np.load(os.path.join(output_dir, 'images.npy'))\n",
        "  #losses = np.load(os.path.join(output_dir, 'losses.npy'))\n",
        "\n",
        "def load_model(device):\n",
        "  \"\"\"\n",
        "  Checkpoint contains:\n",
        "    net_model\n",
        "    ema_model\n",
        "    sched\n",
        "    optim\n",
        "    step\n",
        "  We only need net_model for inference, as the rest are for training the flow model.\n",
        "  \"\"\"\n",
        "  # Create directory where all results will be stored\n",
        "  save_dir = os.path.join(CONFIG['output_dir'], CONFIG['flow_model'])\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "  # Load checkpoint\n",
        "  try:\n",
        "    checkpoint_path = os.path.join(\n",
        "      CONFIG['models_dir'], f\"{CONFIG['flow_model']}_cifar10_weights_step_400000.pt\"\n",
        "    )\n",
        "    print(f\"Attempting to load {CONFIG['flow_model']} model from {checkpoint_path}\")\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, weights_only=True, map_location=device)\n",
        "  except NotImplementedError:\n",
        "    raise NotImplementedError(\n",
        "      f\"Unknown model {CONFIG['flow_model']}, must be one of ['otcfm', 'icfm', 'fm']\"\n",
        "    )\n",
        "  except FileNotFoundError:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Checkpoint file not found at {checkpoint_path}. Verify the path or the file's existence.\"\n",
        "    )\n",
        "\n",
        "  # Create UNet model and load in it's corresponding weights\n",
        "  net_model = net_model = UNetModelWrapper(\n",
        "    dim=(CONFIG['in_channels'], CONFIG['sample_size'], CONFIG['sample_size']),\n",
        "    class_cond=CONFIG['class_cond'],\n",
        "    num_classes=CONFIG['num_classes'],\n",
        "    num_res_blocks=2,\n",
        "    num_channels=128,\n",
        "    channel_mult=[1, 2, 2, 2],\n",
        "    num_heads=4,\n",
        "    num_head_channels=64,\n",
        "    attention_resolutions=\"16\",\n",
        "    dropout=0.1,\n",
        "    use_checkpoint=CONFIG['use_checkpointing'],\n",
        "  ).to(device)\n",
        "\n",
        "  if CONFIG['model_variant'] == 'normal':\n",
        "    net_model.load_state_dict(checkpoint['net_model'])\n",
        "  elif CONFIG['model_variant'] == 'ema':\n",
        "    net_model.load_state_dict(checkpoint['ema_model'])    # kanske borde loada från emea modellen istället?\n",
        "\n",
        "  # For some reason the gradient computatins in our d-flow doesnt work when this is frozen, have to look into that\n",
        "  # Now it seems to work? Odd xD\n",
        "  if CONFIG['frozen_model']:\n",
        "    for name, param in net_model.named_parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  print(f\"Succesfully loaded the {CONFIG['model_variant']} {CONFIG['flow_model']} model onto the {device}.\")\n",
        "  if device == 'cpu': print(f\"Consider switching device to GPU, as it will run out of RAM during D-flow otherwise.\")\n",
        "  return net_model\n",
        "\n",
        "# test_model = load_model(device)"
      ],
      "metadata": {
        "id": "XfcCLOG3LOBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def subsample(mask, percentage=1):\n",
        "  mask = mask.cpu()\n",
        "  H, W = mask.shape\n",
        "  mask_flat = mask.view(H * W)\n",
        "  valid_indices = torch.nonzero(mask_flat).squeeze()\n",
        "  sampled_indices = np.random.choice(valid_indices.numpy(), size=int(len(valid_indices) * percentage), replace=False)\n",
        "  return sampled_indices\n",
        "\n",
        "def mask_area(img, area=(slice(10, 14), slice(10, 14))):\n",
        "  masked_img = torch.clone(img)\n",
        "  mask = torch.ones(img.shape[-2], img.shape[-1]).to(device)\n",
        "  mask[area] = 0\n",
        "  masked_img = masked_img[:, :] * mask\n",
        "  return masked_img, mask\n",
        "\n",
        "def mask_center(img, width=2):\n",
        "  masked_img = torch.clone(img)\n",
        "  mask = torch.ones(img.shape[-2], img.shape[-1]).to(device)\n",
        "  mask[int(img.shape[-2]/2-width):int(img.shape[-2]/2)+width, int(img.shape[-1]/2)-width:int(img.shape[-1]/2)+width] = 0\n",
        "  masked_img = masked_img[:,:] * mask\n",
        "  return masked_img, mask\n",
        "\n",
        "def mask_half(img, width=3):\n",
        "  masked_img = torch.clone(img)\n",
        "  mask = torch.ones(img.shape[-2], img.shape[-1]).to(device)\n",
        "  mask[:,-width:] = 0\n",
        "  masked_img = masked_img[:,:] * mask\n",
        "  return masked_img, mask\n",
        "\n",
        "def mask_gaussian(img, mask_percent=0.2):\n",
        "  mask = np.random.rand(img.shape[-2], img.shape[-1]) > mask_percent\n",
        "  mask = torch.tensor(mask).to(device)\n",
        "  masked_img = img[:,:] * mask\n",
        "  return masked_img, mask\n",
        "\n",
        "def mask_edge(img, width=2):\n",
        "  img = img\n",
        "  masked_img = torch.clone(img)\n",
        "  mask = torch.ones(img.shape[-2], img.shape[-1]).to(device)\n",
        "  mask[:, :width] = 0\n",
        "  mask[:, -width:] = 0\n",
        "  mask[:width, :] = 0\n",
        "  mask[-width:, :] = 0\n",
        "  masked_img = masked_img[:,:] * mask\n",
        "  return masked_img, mask\n",
        "\n",
        "def corrupt_image(image, corruption_type):\n",
        "  if corruption_type == \"mask_area\":\n",
        "    return mask_area(image, CONFIG['inpaint_param'])\n",
        "  elif corruption_type == \"mask_center\":\n",
        "    return mask_center(image, CONFIG['inpaint_param'])\n",
        "  elif corruption_type == \"mask_half\":\n",
        "    return mask_half(image, CONFIG['inpaint_param'])\n",
        "  elif corruption_type == \"mask_gaussian\":\n",
        "    return mask_gaussian(image, CONFIG['inpaint_param'])\n",
        "  elif corruption_type == \"mask_edge\":\n",
        "    return mask_edge(image, CONFIG['inpaint_param'])\n",
        "\n",
        "\n",
        "\n",
        "def inpainting_loss(x,y,mask):\n",
        "  B, C, H, W = x.shape\n",
        "\n",
        "  # Flatten the image as the mask is flat\n",
        "  x_flat, y_flat = x.view(B, C, H*W), y.view(B, C, H*W)\n",
        "\n",
        "  # Retrieve the values at the sampled indices\n",
        "  x_sampled_pixels = x_flat[:, :, mask]\n",
        "  y_sampled_pixels = y_flat[:, :, mask]\n",
        "\n",
        "  # Return the loss\n",
        "  return torch.mean((x_sampled_pixels - y_sampled_pixels)**2)\n",
        "\n",
        "\n",
        "def display_sample(sample, i):\n",
        "  image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "  image_processed = (image_processed + 1.0) * 127.5\n",
        "  image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "  image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "  display(f\"Image at step {i}\")\n",
        "  display(image_pil)\n",
        "\n",
        "def plot_d_flow_process(x_1_list, img_per_row):\n",
        "  test_images = []\n",
        "  for i, images in enumerate(x_1_list):\n",
        "    images = images[-1].view([3, 32, 32]).clip(-1, 1)\n",
        "    images = images / 2 + 0.5\n",
        "    test_images.append(images)\n",
        "\n",
        "  grid = make_grid(test_images, value_range=(0, 1), nrow=img_per_row, padding=2)\n",
        "  img = ToPILImage()(grid)\n",
        "  plt.imshow(img)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_losses(losses):\n",
        "  all_loses = []\n",
        "  for sublist in losses:\n",
        "    for loss in sublist:\n",
        "      all_loses.append(loss.item())\n",
        "\n",
        "  # Plot the loss curve\n",
        "  plt.plot(all_loses)\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(f\"Loss vs Iteration\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "QnMZNmBiLPiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQ1qMuD8LQqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_x_0(model, corrupted_y):\n",
        "  x_0 = torch.randn(\n",
        "   1, CONFIG['in_channels'], CONFIG['sample_size'], CONFIG['sample_size'], device=device, requires_grad=True\n",
        "  )\n",
        "  ODE_func = lambda t, x: model.forward(t, x)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    x_0 = torchdiffeq.odeint(\n",
        "      func=ODE_func,\n",
        "      y0=corrupted_y,\n",
        "      t=torch.linspace(1, 0, CONFIG['ODE_steps'], device=device),\n",
        "      atol=1e-4,\n",
        "      rtol=1e-4,\n",
        "      method=CONFIG['ODE_type'],\n",
        "    )[-1].to(device)\n",
        "\n",
        "  x_0 = CONFIG['blend_param'] **0.5 * (corrupted_y + x_0) + (1. - CONFIG['blend_param'])**0.5*torch.randn_like(x_0)\n",
        "  x_0.requires_grad=True\n",
        "  return x_0\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "def setup_experiment_dirs(base_dir, experiment_id):\n",
        "  log_dir = os.path.join(base_dir, f'experiment_{experiment_id}/logs')\n",
        "  output_dir = os.path.join(base_dir, f'experiment_{experiment_id}')\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  return log_dir, output_dir\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "def save_PIL_images(path, output_dict):\n",
        "  os.makedirs(path, exist_ok=True)\n",
        "  D_flow_images = [output_dict[image]['x_1_list'][-1][-1] for image in output_dict.keys()]\n",
        "  denormalized_D_flow_images = denormalize_images(D_flow_images)\n",
        "\n",
        "  for idx, image_tensor in enumerate(denormalized_D_flow_images):\n",
        "    pil_image = TF.to_pil_image(image_tensor.squeeze(0))\n",
        "\n",
        "    image_file_path = os.path.join(path, f'd_flow_image_{idx}.jpg')\n",
        "    pil_image.save(image_file_path)\n",
        "\n",
        "def evaluate_d_flow(model, N, corruption_type='inpainting'):\n",
        "  base_dir = '/content/drive/MyDrive/Mac pro dator googl drive/Universitet/KTH/DD2412/Project/Runs'\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "  experiment_id = len(os.listdir(base_dir)) + 2\n",
        "  print(f\"Experiment {experiment_id}\")\n",
        "  log_dir, output_dir = setup_experiment_dirs(base_dir, experiment_id)\n",
        "  config_file = os.path.join(base_dir, f'experiment_{experiment_id}/config.yaml')\n",
        "\n",
        "  save_config(CONFIG, config_file)\n",
        "\n",
        "  writer = SummaryWriter(log_dir)\n",
        "\n",
        "  output_dict = {}\n",
        "  for i in range(CONFIG['num_images']):\n",
        "    output_dict[i] = {}\n",
        "\n",
        "    y = loaded_single_class_images[i].view(1,3,32,32).to(device)\n",
        "    test_y = y.clone()\n",
        "    test_y = test_y.view([3, 32, 32]).clip(-1, 1)\n",
        "    output_dict[i]['y'] = test_y\n",
        "\n",
        "    writer.add_image('Ground Truth', denormalize_images(test_y.squeeze(0)), i)\n",
        "\n",
        "    x_0_list, image_list, losses = d_flow(model, y, N, writer, output_dict, corruption_type, img_num=i)\n",
        "\n",
        "    grid = create_img_grid([images[-1].view([3,32,32]).clip(-1,1) for images in image_list+[y]], _nrow=3, _padding=2)\n",
        "\n",
        "    writer.add_image('D-Flow Step Images', grid, i)\n",
        "\n",
        "  save_output(os.path.join(output_dir, 'output_dict.pt'), output_dict)\n",
        "  save_PIL_images(os.path.join(output_dir, 'd_flow_images'), output_dict)\n",
        "\n",
        "def d_flow(model, y, N, writer, output_dict, corruption_type='inpainting', img_num=0):\n",
        "  corrupted_y, mask = corrupt_image(y, corruption_type)\n",
        "  corrupted_y = corrupted_y.to(device)\n",
        "  subsamples = subsample(mask, 0.9)\n",
        "  corrupted_y.requires_grad = False\n",
        "\n",
        "  test_corrupted_y = corrupted_y.clone()\n",
        "  test_corrupted_y = test_corrupted_y.view([3, 32, 32]).clip(-1, 1)\n",
        "\n",
        "  output_dict[img_num][\"Corrupted Y\"] = test_corrupted_y\n",
        "  writer.add_image('Corrupted Y', denormalize_images(test_corrupted_y), img_num)\n",
        "\n",
        "\n",
        "  ODE_func = lambda t, x: model.forward(t, x)\n",
        "\n",
        "  x_0 = generate_x_0(model, corrupted_y)\n",
        "\n",
        "  optim = torch.optim.LBFGS(\n",
        "    [x_0],\n",
        "    lr=CONFIG['lr'],\n",
        "    max_iter=CONFIG['max_iter'],\n",
        "    history_size=20,\n",
        "    tolerance_grad=1e-7,\n",
        "    tolerance_change=1e-9,\n",
        "    line_search_fn=\"strong_wolfe\"\n",
        "  )\n",
        "\n",
        "  #optim = torch.optim.Adagrad([x_0], lr=CONFIG['lr'])\n",
        "  \"\"\"\n",
        "  optim = torch_optimizer.Adahessian(\n",
        "        [x_0],\n",
        "        lr=CONFIG['lr'],\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-4,\n",
        "        weight_decay=0.0,\n",
        "        hessian_power=1.0,\n",
        "    )\n",
        "  \"\"\"\n",
        "  scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=CONFIG['gamma'])\n",
        "\n",
        "\n",
        "  total_steps = 0\n",
        "\n",
        "  x_0_list, image_list, losses = [], [], []\n",
        "\n",
        "  closure_counter = 0\n",
        "\n",
        "  for i in tqdm.tqdm(range(N)):\n",
        "    iter_losses, iter_x_0s, iter_x_1s = [], [], []\n",
        "\n",
        "    def closure():\n",
        "      optim.zero_grad()\n",
        "\n",
        "      x_1 = torchdiffeq.odeint(\n",
        "        func=ODE_func,\n",
        "        y0=x_0,\n",
        "        t=torch.linspace(0, 1, CONFIG['ODE_steps'], device=device), #\n",
        "        atol=1e-4,\n",
        "        rtol=1e-4,\n",
        "        method=CONFIG['ODE_type'],\n",
        "      )[-1]\n",
        "\n",
        "      loss = inpainting_loss(x_1, corrupted_y, subsamples)\n",
        "\n",
        "\n",
        "      iter_losses.append(loss.item())\n",
        "      iter_x_0s.append(x_0.detach())\n",
        "      iter_x_1s.append(x_1.detach())\n",
        "\n",
        "      #loss.backward(create_graph=True) for AdaHessian'\n",
        "      loss.backward()\n",
        "\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_([x_0], max_norm=1) #TODO\n",
        "\n",
        "      return loss\n",
        "\n",
        "    optim.step(closure)\n",
        "    scheduler.step() #TODO\n",
        "\n",
        "    #for param in [x_0]: #for AdaHessian\n",
        "    #  param.grad = None #for AdaHessian\n",
        "\n",
        "    losses.append(iter_losses)\n",
        "    x_0_list.append(iter_x_0s)\n",
        "    image_list.append(iter_x_1s)\n",
        "\n",
        "    # Logging code\n",
        "    grid = create_img_grid([image.view([3,32,32]).clip(-1,1) for image in iter_x_1s])\n",
        "\n",
        "    writer.add_image(f'Closure Images/img_{img_num}', grid, closure_counter)\n",
        "    closure_counter += 1\n",
        "\n",
        "  flattened_data = [item for sublist in losses for item in sublist]\n",
        "\n",
        "  plt.figure(figsize=(8, 6))\n",
        "\n",
        "  plt.plot(range(len(flattened_data)), flattened_data, marker='o', label='Loss')\n",
        "\n",
        "  plt.yscale('log')\n",
        "  plt.xlabel('Steps')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  output_dict[img_num]['x_0_list'] = x_0_list\n",
        "  output_dict[img_num]['x_1_list'] = image_list\n",
        "  output_dict[img_num]['losses'] = losses\n",
        "  return x_0_list, image_list, losses\n"
      ],
      "metadata": {
        "id": "GFcbkrHVLR48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_model=load_model(device)"
      ],
      "metadata": {
        "id": "CSUZwbElLTfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/drive/MyDrive/Mac pro dator googl drive/Universitet/KTH/DD2412/Project/Runs'\n"
      ],
      "metadata": {
        "id": "2XSVv7uPLUlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify config as needed\n",
        "net_model=load_model(device)\n",
        "evaluate_d_flow(net_model, CONFIG['total_iter'], corruption_type=CONFIG['corrupt_Type'])"
      ],
      "metadata": {
        "id": "eaoePMGCLanj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}